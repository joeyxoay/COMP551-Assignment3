# -*- coding: utf-8 -*-
"""Results.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GeDFA7nIBef323CZ13Rp4mI9OHtlbwnB

# **Setup and Data Preparation**
"""

# Import necessary libraries
import torch
from transformers import BertTokenizer, BertForSequenceClassification, BertConfig
from datasets import load_dataset
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
dataset = load_dataset("dair-ai/emotion")
test_data = dataset["test"]

# Load the tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
config = BertConfig.from_pretrained('bert-base-uncased', num_labels=6, output_attentions=True)
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)
model.eval()  # Set the model to evaluation mode

"""# **Prediction and Attention Analysis**"""

# Define the prediction function
def predict_with_attention(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    outputs = model(**inputs)
    return outputs.logits, outputs.attentions

# Define the attention analysis function
def analyze_attention(attentions, input_ids, tokenizer, head_num, block_num):
    attention_matrix = attentions[block_num].squeeze(0)
    padding_token_idx = tokenizer.pad_token_id
    padding_mask = (input_ids != padding_token_idx).unsqueeze(-1)
    masked_attention = attention_matrix * padding_mask
    cls_attention = masked_attention[head_num][0, :]
    attention_sum = cls_attention.sum()
    normalized_attention = cls_attention / attention_sum if attention_sum > 0 else cls_attention
    return normalized_attention.cpu().detach().numpy()

# Analysis Functions

def attention_to_key_words(attention_weights, tokens, key_words):
    """Analyzes attention given to specified key words."""
    attention_scores = {}
    for word in key_words:
        if word in tokens:
            idx = tokens.index(word)
            attention_scores[word] = attention_weights[idx]
    return attention_scores

emotion_words = ['happy', 'sad', 'angry', 'excited', 'joyful', 'elated', 'thrilled', 'ecstatic', 'euphoric',
                 'depressed', 'gloomy', 'melancholic', 'mournful', 'irritated', 'annoyed', 'furious', 'rageful',
                 'enthusiastic', 'eager', 'animated', 'spirited', 'fearful', 'scared', 'terrified', 'anxious',
                 'apprehensive', 'surprised', 'astonished', 'amazed', 'shocked', 'startled', 'disgusted',
                 'revolted', 'repulsed', 'sickened', 'nauseated', 'content', 'satisfied', 'pleased', 'comfortable',
                 'relaxed', 'hopeful', 'optimistic', 'positive', 'upbeat', 'confident', 'confused', 'perplexed',
                 'baffled', 'bewildered', 'puzzled']  # Extend this list with emotion-related words

def calculate_attention_stats(attention_weights):
    """Calculates mean and standard deviation of attention weights."""
    return np.mean(attention_weights), np.std(attention_weights)

"""# **Heat Map visual**"""

import matplotlib.pyplot as plt
import seaborn as sns

# Define the heatmap plotting function
def plot_attention_heatmap(attention_weights, tokens):
    plt.figure(figsize=(20, 1))
    sns.heatmap(attention_weights[None, :], cmap='viridis', xticklabels=tokens, yticklabels=False, cbar=True)
    plt.xticks(rotation=90)
    plt.show()

"""# **Run Experiment and Analyze**

**Setup for Experiment and Analysis**
"""

# Define emotion-related words for analysis
emotion_words = [
    'happy', 'sad', 'angry', 'excited', 'joyful', 'elated', 'thrilled', 'ecstatic', 'euphoric',
    'depressed', 'gloomy', 'melancholic', 'mournful', 'irritated', 'annoyed', 'furious', 'rageful',
    'enthusiastic', 'eager', 'animated', 'spirited', 'fearful', 'scared', 'terrified', 'anxious',
    'apprehensive', 'surprised', 'astonished', 'amazed', 'shocked', 'startled', 'disgusted',
    'revolted', 'repulsed', 'sickened', 'nauseated', 'content', 'satisfied', 'pleased', 'comfortable',
    'relaxed', 'hopeful', 'optimistic', 'positive', 'upbeat', 'confident', 'confused', 'perplexed',
    'baffled', 'bewildered', 'puzzled', 'rotten', 'ambitious', 'uncomfortable', 'anger', 'immature',
    'excitement', 'grumpy', 'agitated', 'invaded', 'helpless', 'supportive', 'mellow', 'outraged',
    'insecure', 'restless', 'productive', 'peaceful', 'welcomed', 'virtuous', 'delighted', 'affectionate'
    'grumpy','mellow','tired','mad', 'rude', 'virtuous', 'honoured', 'reassured', 'friendly'
]

# Containers for post-experiment analysis
attention_details = []
emotion_word_attention = {word: [] for word in emotion_words}
correct_predictions_attention = []
incorrect_predictions_attention = []

"""**Experiment Loop with Analysis and Heatmap Visualization**"""

# Loop over the selected test data
subset_size = 50  # Adjust the range as needed
subset_test_data = test_data.select(range(subset_size))

for example in subset_test_data:
    text = example['text']
    input_ids = tokenizer.encode(text, return_tensors='pt')

    logits, attentions = predict_with_attention(text)
    predicted_label = logits.argmax(dim=1).item()
    actual_label = example['label']

    attention_matrix = analyze_attention(attentions, input_ids, tokenizer, head_num=0, block_num=11)
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
    plot_attention_heatmap(attention_matrix, tokens)

    # Additional Analysis
    key_word_attention = attention_to_key_words(attention_matrix, tokens, emotion_words)
    mean_att, std_att = calculate_attention_stats(attention_matrix)

    # Collecting data for post-experiment analysis
    attention_details.append({
        "text": text,
        "predicted_label": predicted_label,
        "actual_label": actual_label,
        "key_word_attention": key_word_attention,
        "mean_attention": mean_att,
        "std_attention": std_att
    })

    # Separate collection for correct and incorrect predictions
    if predicted_label == actual_label:
        correct_predictions_attention.append(mean_att)
    else:
        incorrect_predictions_attention.append(mean_att)

    # Collect attention to emotion words
    for word, attention in key_word_attention.items():
        emotion_word_attention[word].append(attention)

"""**Post Experiment Analysis**"""

# Analyze the collected data

# Average attention to emotion words
average_attention_per_word = {word: np.mean(attentions) for word, attentions in emotion_word_attention.items() if attentions}
print("Average attention to key emotion words:", average_attention_per_word)

# Comparison of attention in correct vs. incorrect predictions
print("Average attention for correct predictions:", np.mean(correct_predictions_attention))
print("Average attention for incorrect predictions:", np.mean(incorrect_predictions_attention))

# Error analysis on incorrect predictions
for detail in attention_details:
    if detail['predicted_label'] != detail['actual_label']:
        print(f"Text: {detail['text']}")
        print(f"Predicted: {detail['predicted_label']}, Actual: {detail['actual_label']}")
        print(f"Attention to key emotion words: {detail['key_word_attention']}\n")

"""**Additional Experiment: Distribution of Attention Across Different Sentence Structures**"""

from collections import defaultdict

def categorize_sentence_structure(text):
    # Example categorization based on sentence length
    length = len(text.split())
    if length <= 10:
        return 'short'
    elif length <= 20:
        return 'medium'
    else:
        return 'long'

def analyze_sentence_structure_attention(subset_test_data):
    structure_attention_stats = []

    for example in subset_test_data:
        text = example['text']
        input_ids = tokenizer.encode(text, return_tensors='pt')
        _, attentions = predict_with_attention(text)
        attention_matrix = analyze_attention(attentions, input_ids, tokenizer, head_num=0, block_num=11)
        sentence_length = len(input_ids[0])
        mean_attention = np.mean(attention_matrix)

        # Add your logic to categorize sentence structures (e.g., simple, complex)
        sentence_structure = categorize_sentence_structure(text)  # Implement this function based on your criteria
        structure_attention_stats.append((sentence_structure, sentence_length, mean_attention))

    return structure_attention_stats

# Analyze and plot after experiment loop
structure_attention_stats = analyze_sentence_structure_attention(subset_test_data)
# Implement plotting or statistical analysis based on structure_attention_stats

# Initialize dictionaries to store aggregated data
attention_aggregate = defaultdict(list)

# Aggregate data
for structure, _, mean_attention in structure_attention_stats:
    attention_aggregate[structure].append(mean_attention)

# Calculate average attention for each sentence structure
average_attention_by_structure = {structure: np.mean(attentions) for structure, attentions in attention_aggregate.items()}

# Extracting data for plotting
structures = list(average_attention_by_structure.keys())
average_attentions = list(average_attention_by_structure.values())

# Plotting
plt.bar(structures, average_attentions, color=['blue', 'green', 'red'])
plt.xlabel('Sentence Structure')
plt.ylabel('Average Attention')
plt.title('Average Attention by Sentence Structure')
plt.show()

"""**Additional Experiment2: Attention Entropy Analysis**"""

from scipy.stats import entropy

def calculate_attention_entropy(attention_weights):
    return entropy(attention_weights)

# Collect entropy data in the experiment loop
attention_entropies = []
for detail in attention_details:
    attention_entropy = calculate_attention_entropy(detail['mean_attention'])
    attention_entropies.append(attention_entropy)

# Analyze the collected entropies
print("Average attention entropy:", np.mean(attention_entropies))

"""# **Different Head and Block Position Experiments**

**Different Head And Block (Block 1; head 0)**
"""

# Loop over the selected test data
subset_size = 50  # Adjust the range as needed
subset_test_data = test_data.select(range(subset_size))

for example in subset_test_data:
    text = example['text']
    input_ids = tokenizer.encode(text, return_tensors='pt')

    logits, attentions = predict_with_attention(text)
    predicted_label = logits.argmax(dim=1).item()
    actual_label = example['label']

    attention_matrix = analyze_attention(attentions, input_ids, tokenizer, head_num=0, block_num=1)
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
    plot_attention_heatmap(attention_matrix, tokens)

    # Additional Analysis
    key_word_attention = attention_to_key_words(attention_matrix, tokens, emotion_words)
    mean_att, std_att = calculate_attention_stats(attention_matrix)

    # Collecting data for post-experiment analysis
    attention_details.append({
        "text": text,
        "predicted_label": predicted_label,
        "actual_label": actual_label,
        "key_word_attention": key_word_attention,
        "mean_attention": mean_att,
        "std_attention": std_att
    })

    # Separate collection for correct and incorrect predictions
    if predicted_label == actual_label:
        correct_predictions_attention.append(mean_att)
    else:
        incorrect_predictions_attention.append(mean_att)

    # Collect attention to emotion words
    for word, attention in key_word_attention.items():
        emotion_word_attention[word].append(attention)


# Analyze the collected data

# Average attention to emotion words
average_attention_per_word = {word: np.mean(attentions) for word, attentions in emotion_word_attention.items() if attentions}
print("Average attention to key emotion words:", average_attention_per_word)

# Comparison of attention in correct vs. incorrect predictions
print("Average attention for correct predictions:", np.mean(correct_predictions_attention))
print("Average attention for incorrect predictions:", np.mean(incorrect_predictions_attention))

# Error analysis on incorrect predictions
for detail in attention_details:
    if detail['predicted_label'] != detail['actual_label']:
        print(f"Text: {detail['text']}")
        print(f"Predicted: {detail['predicted_label']}, Actual: {detail['actual_label']}")
        print(f"Attention to key emotion words: {detail['key_word_attention']}\n")

"""**Different Location: Head 3; Block 2**"""

# Loop over the selected test data
subset_size = 50  # Adjust the range as needed
subset_test_data = test_data.select(range(subset_size))

for example in subset_test_data:
    text = example['text']
    input_ids = tokenizer.encode(text, return_tensors='pt')

    logits, attentions = predict_with_attention(text)
    predicted_label = logits.argmax(dim=1).item()
    actual_label = example['label']

    attention_matrix = analyze_attention(attentions, input_ids, tokenizer, head_num=3, block_num=2)
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
    plot_attention_heatmap(attention_matrix, tokens)

    # Additional Analysis
    key_word_attention = attention_to_key_words(attention_matrix, tokens, emotion_words)
    mean_att, std_att = calculate_attention_stats(attention_matrix)

    # Collecting data for post-experiment analysis
    attention_details.append({
        "text": text,
        "predicted_label": predicted_label,
        "actual_label": actual_label,
        "key_word_attention": key_word_attention,
        "mean_attention": mean_att,
        "std_attention": std_att
    })

    # Separate collection for correct and incorrect predictions
    if predicted_label == actual_label:
        correct_predictions_attention.append(mean_att)
    else:
        incorrect_predictions_attention.append(mean_att)

    # Collect attention to emotion words
    for word, attention in key_word_attention.items():
        emotion_word_attention[word].append(attention)


# Analyze the collected data

# Average attention to emotion words
average_attention_per_word = {word: np.mean(attentions) for word, attentions in emotion_word_attention.items() if attentions}
print("Average attention to key emotion words:", average_attention_per_word)

# Comparison of attention in correct vs. incorrect predictions
print("Average attention for correct predictions:", np.mean(correct_predictions_attention))
print("Average attention for incorrect predictions:", np.mean(incorrect_predictions_attention))

# Error analysis on incorrect predictions
for detail in attention_details:
    if detail['predicted_label'] != detail['actual_label']:
        print(f"Text: {detail['text']}")
        print(f"Predicted: {detail['predicted_label']}, Actual: {detail['actual_label']}")
        print(f"Attention to key emotion words: {detail['key_word_attention']}\n")

"""**Head at 4; block at 4**"""

# Loop over the selected test data
subset_size = 50  # Adjust the range as needed
subset_test_data = test_data.select(range(subset_size))

for example in subset_test_data:
    text = example['text']
    input_ids = tokenizer.encode(text, return_tensors='pt')

    logits, attentions = predict_with_attention(text)
    predicted_label = logits.argmax(dim=1).item()
    actual_label = example['label']

    attention_matrix = analyze_attention(attentions, input_ids, tokenizer, head_num=4, block_num=4)
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
    plot_attention_heatmap(attention_matrix, tokens)

    # Additional Analysis
    key_word_attention = attention_to_key_words(attention_matrix, tokens, emotion_words)
    mean_att, std_att = calculate_attention_stats(attention_matrix)

    # Collecting data for post-experiment analysis
    attention_details.append({
        "text": text,
        "predicted_label": predicted_label,
        "actual_label": actual_label,
        "key_word_attention": key_word_attention,
        "mean_attention": mean_att,
        "std_attention": std_att
    })

    # Separate collection for correct and incorrect predictions
    if predicted_label == actual_label:
        correct_predictions_attention.append(mean_att)
    else:
        incorrect_predictions_attention.append(mean_att)

    # Collect attention to emotion words
    for word, attention in key_word_attention.items():
        emotion_word_attention[word].append(attention)


# Analyze the collected data

# Average attention to emotion words
average_attention_per_word = {word: np.mean(attentions) for word, attentions in emotion_word_attention.items() if attentions}
print("Average attention to key emotion words:", average_attention_per_word)

# Comparison of attention in correct vs. incorrect predictions
print("Average attention for correct predictions:", np.mean(correct_predictions_attention))
print("Average attention for incorrect predictions:", np.mean(incorrect_predictions_attention))

# Error analysis on incorrect predictions
for detail in attention_details:
    if detail['predicted_label'] != detail['actual_label']:
        print(f"Text: {detail['text']}")
        print(f"Predicted: {detail['predicted_label']}, Actual: {detail['actual_label']}")
        print(f"Attention to key emotion words: {detail['key_word_attention']}\n")

"""**Head:8; Block:8**"""

# Loop over the selected test data
subset_size = 50  # Adjust the range as needed
subset_test_data = test_data.select(range(subset_size))

for example in subset_test_data:
    text = example['text']
    input_ids = tokenizer.encode(text, return_tensors='pt')

    logits, attentions = predict_with_attention(text)
    predicted_label = logits.argmax(dim=1).item()
    actual_label = example['label']

    attention_matrix = analyze_attention(attentions, input_ids, tokenizer, head_num=8, block_num=8)
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
    plot_attention_heatmap(attention_matrix, tokens)

    # Additional Analysis
    key_word_attention = attention_to_key_words(attention_matrix, tokens, emotion_words)
    mean_att, std_att = calculate_attention_stats(attention_matrix)

    # Collecting data for post-experiment analysis
    attention_details.append({
        "text": text,
        "predicted_label": predicted_label,
        "actual_label": actual_label,
        "key_word_attention": key_word_attention,
        "mean_attention": mean_att,
        "std_attention": std_att
    })

    # Separate collection for correct and incorrect predictions
    if predicted_label == actual_label:
        correct_predictions_attention.append(mean_att)
    else:
        incorrect_predictions_attention.append(mean_att)

    # Collect attention to emotion words
    for word, attention in key_word_attention.items():
        emotion_word_attention[word].append(attention)


# Analyze the collected data

# Average attention to emotion words
average_attention_per_word = {word: np.mean(attentions) for word, attentions in emotion_word_attention.items() if attentions}
print("Average attention to key emotion words:", average_attention_per_word)

# Comparison of attention in correct vs. incorrect predictions
print("Average attention for correct predictions:", np.mean(correct_predictions_attention))
print("Average attention for incorrect predictions:", np.mean(incorrect_predictions_attention))

# Error analysis on incorrect predictions
for detail in attention_details:
    if detail['predicted_label'] != detail['actual_label']:
        print(f"Text: {detail['text']}")
        print(f"Predicted: {detail['predicted_label']}, Actual: {detail['actual_label']}")
        print(f"Attention to key emotion words: {detail['key_word_attention']}\n")

"""**Head:2 ; Block:11**"""

# Loop over the selected test data
subset_size = 50  # Adjust the range as needed
subset_test_data = test_data.select(range(subset_size))

for example in subset_test_data:
    text = example['text']
    input_ids = tokenizer.encode(text, return_tensors='pt')

    logits, attentions = predict_with_attention(text)
    predicted_label = logits.argmax(dim=1).item()
    actual_label = example['label']

    attention_matrix = analyze_attention(attentions, input_ids, tokenizer, head_num=2, block_num=11)
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
    plot_attention_heatmap(attention_matrix, tokens)

    # Additional Analysis
    key_word_attention = attention_to_key_words(attention_matrix, tokens, emotion_words)
    mean_att, std_att = calculate_attention_stats(attention_matrix)

    # Collecting data for post-experiment analysis
    attention_details.append({
        "text": text,
        "predicted_label": predicted_label,
        "actual_label": actual_label,
        "key_word_attention": key_word_attention,
        "mean_attention": mean_att,
        "std_attention": std_att
    })

    # Separate collection for correct and incorrect predictions
    if predicted_label == actual_label:
        correct_predictions_attention.append(mean_att)
    else:
        incorrect_predictions_attention.append(mean_att)

    # Collect attention to emotion words
    for word, attention in key_word_attention.items():
        emotion_word_attention[word].append(attention)


# Analyze the collected data

# Average attention to emotion words
average_attention_per_word = {word: np.mean(attentions) for word, attentions in emotion_word_attention.items() if attentions}
print("Average attention to key emotion words:", average_attention_per_word)

# Comparison of attention in correct vs. incorrect predictions
print("Average attention for correct predictions:", np.mean(correct_predictions_attention))
print("Average attention for incorrect predictions:", np.mean(incorrect_predictions_attention))

# Error analysis on incorrect predictions
for detail in attention_details:
    if detail['predicted_label'] != detail['actual_label']:
        print(f"Text: {detail['text']}")
        print(f"Predicted: {detail['predicted_label']}, Actual: {detail['actual_label']}")
        print(f"Attention to key emotion words: {detail['key_word_attention']}\n")

"""**Head:8 ; Block 11**"""

# Loop over the selected test data
subset_size = 50  # Adjust the range as needed
subset_test_data = test_data.select(range(subset_size))

for example in subset_test_data:
    text = example['text']
    input_ids = tokenizer.encode(text, return_tensors='pt')

    logits, attentions = predict_with_attention(text)
    predicted_label = logits.argmax(dim=1).item()
    actual_label = example['label']

    attention_matrix = analyze_attention(attentions, input_ids, tokenizer, head_num=8, block_num=11)
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
    plot_attention_heatmap(attention_matrix, tokens)

    # Additional Analysis
    key_word_attention = attention_to_key_words(attention_matrix, tokens, emotion_words)
    mean_att, std_att = calculate_attention_stats(attention_matrix)

    # Collecting data for post-experiment analysis
    attention_details.append({
        "text": text,
        "predicted_label": predicted_label,
        "actual_label": actual_label,
        "key_word_attention": key_word_attention,
        "mean_attention": mean_att,
        "std_attention": std_att
    })

    # Separate collection for correct and incorrect predictions
    if predicted_label == actual_label:
        correct_predictions_attention.append(mean_att)
    else:
        incorrect_predictions_attention.append(mean_att)

    # Collect attention to emotion words
    for word, attention in key_word_attention.items():
        emotion_word_attention[word].append(attention)


# Analyze the collected data

# Average attention to emotion words
average_attention_per_word = {word: np.mean(attentions) for word, attentions in emotion_word_attention.items() if attentions}
print("Average attention to key emotion words:", average_attention_per_word)

# Comparison of attention in correct vs. incorrect predictions
print("Average attention for correct predictions:", np.mean(correct_predictions_attention))
print("Average attention for incorrect predictions:", np.mean(incorrect_predictions_attention))

# Error analysis on incorrect predictions
for detail in attention_details:
    if detail['predicted_label'] != detail['actual_label']:
        print(f"Text: {detail['text']}")
        print(f"Predicted: {detail['predicted_label']}, Actual: {detail['actual_label']}")
        print(f"Attention to key emotion words: {detail['key_word_attention']}\n")